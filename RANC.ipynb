{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8Y9pWN8pIFP",
        "outputId": "9a8347ca-c3b9-4515-d25a-e4671f27c373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtguNxFbOU8c",
        "outputId": "1f710490-2af3-448c-cf42-4aab8febe1ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/RANC/software\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/RANC/software"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM0_VIVyJrL6",
        "outputId": "dd138bfb-68b8-4ec5-f1ff-a590d2e62c06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./tealayers/tealayer2.0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from tealayer2==2.0) (2.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tealayer2==2.0) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tealayer2==2.0) (9.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (4.12.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->tealayer2==2.0) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->tealayer2==2.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->tealayer2==2.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->tealayer2==2.0) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->tealayer2==2.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->tealayer2==2.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->tealayer2==2.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->tealayer2==2.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->tealayer2==2.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->tealayer2==2.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->tealayer2==2.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->tealayer2==2.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->tealayer2==2.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->tealayer2==2.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->tealayer2==2.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->tealayer2==2.0) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow->tealayer2==2.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->tealayer2==2.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->tealayer2==2.0) (3.2.2)\n",
            "Building wheels for collected packages: tealayer2\n",
            "  Building wheel for tealayer2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tealayer2: filename=tealayer2-2.0-py3-none-any.whl size=8768 sha256=ef315c6ee7d8165fc1a63d0266647fbe68e451594de94725ce25501ad2d1138b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-69vlr7c1/wheels/8b/52/4c/1dcaed17af4c15154718083a8af454a895a4a4d3528c690b18\n",
            "Successfully built tealayer2\n",
            "Installing collected packages: tealayer2\n",
            "  Attempting uninstall: tealayer2\n",
            "    Found existing installation: tealayer2 2.0\n",
            "    Uninstalling tealayer2-2.0:\n",
            "      Successfully uninstalled tealayer2-2.0\n",
            "Successfully installed tealayer2-2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ./tealayers/tealayer2.0/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjbgS5Mp0fXW",
        "outputId": "395604bd-9885-4ce7-80ff-7db73736f93c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./rancutils\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rancutils==0.1) (1.25.2)\n",
            "Requirement already satisfied: bitstring in /usr/local/lib/python3.10/dist-packages (from rancutils==0.1) (4.2.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from rancutils==0.1) (2.31.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from rancutils==0.1) (2.0.3)\n",
            "Requirement already satisfied: bitarray<3.0.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from bitstring->rancutils==0.1) (2.9.2)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio->rancutils==0.1) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->rancutils==0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->rancutils==0.1) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->rancutils==0.1) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->rancutils==0.1) (1.16.0)\n",
            "Building wheels for collected packages: rancutils\n",
            "  Building wheel for rancutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rancutils: filename=rancutils-0.1-py3-none-any.whl size=12918 sha256=7d792efcebff0cdfd5f9bcf5aeaf11844e8bea1a77cf7325d5d716a119d2a247\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_k_a1q38/wheels/c1/2c/b7/78cae1a5e6b1ec6c0ce2a87e8df680c25c2a9d80dc258c23f1\n",
            "Successfully built rancutils\n",
            "Installing collected packages: rancutils\n",
            "  Attempting uninstall: rancutils\n",
            "    Found existing installation: rancutils 0.1\n",
            "    Uninstalling rancutils-0.1:\n",
            "      Successfully uninstalled rancutils-0.1\n",
            "Successfully installed rancutils-0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install ./rancutils/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fmLL7QPC9lk",
        "outputId": "defe2a89-545f-46dc-ae98-efb091674d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from tealayer2 import Tea, AdditivePooling\n",
        "from tensorflow.keras.layers import Flatten, Activation, Input, Lambda, concatenate\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import Model\n",
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "tf.disable_v2_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9GhM3hWBW9aA"
      },
      "outputs": [],
      "source": [
        "num_test_samples = 10000"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Greyscale images are of shape (28,28,1)\n",
        "inputs = Input(shape=(28,28,1))\n",
        "\n",
        "# Flatten the inputs so that inputs map as: flatten_input[0] -> axon[0], ..., flatten_input[255] -> axon[255]\n",
        "flattened_inputs = Flatten()(inputs)\n",
        "\n",
        "# Generate each core.\n",
        "# We are taking a 16x16 square of the input image and striding it by 12. this gives us 4 cores with 0 padding encumpassing the entire image.\n",
        "# Trải phẳng các ảnh input 28x28x1 thành 1 vector 784x1, quét từng đoạt 256 để đưa vào lần lượt các core, mỗi đoạn sau stride 12 phần tử so với đoạn trước\n",
        "# (Trong chương 02 ở file doc là quét 4 góc bức ảnh, nó cũng tương tự như thế này)\n",
        "core0 = Lambda(lambda x : x[:, :256])(flattened_inputs)\n",
        "core1 = Lambda(lambda x : x[:, 176:432])(flattened_inputs)\n",
        "core2 = Lambda(lambda x : x[:, 352:608])(flattened_inputs)\n",
        "core3 = Lambda(lambda x : x[:, 528:])(flattened_inputs)\n",
        "\n",
        "# Use the image distributions as corresponding inputs into our Tea Layer.\n",
        "# units là số neuron được sử dụng cho tea layer này, ở đây khai báo 4 tea layer với units = 64 (tương ứng 4 core với mỗi core sử dụng 64 neuron)\n",
        "core0 = Tea(units=128, name='tea_1_1')(core0)\n",
        "core1 = Tea(units=128, name='tea_1_2')(core1)\n",
        "core2 = Tea(units=128, name='tea_1_3')(core2)\n",
        "core3 = Tea(units=128, name='tea_1_4')(core3)\n",
        "\n",
        "\n",
        "# The classification is the concatenation of these 4 core's outputs.\n",
        "# We'll call the classification core our 'network'\n",
        "layer1 = concatenate([core0, core1, core2, core3])\n",
        "\n",
        "# network0 = Lambda(lambda x: concatenate([x[:1, :64], x[1:2, 128:192], x[2:3, 256:320], x[3:4, 384:448]], axis = -1))(layer1)\n",
        "# network1 = Lambda(lambda x: concatenate([x[:1, 64:128], x[1:2, 192:256], x[2:3, 320:384], x[3:4, 448:]], axis = -1))(layer1)\n",
        "network0 =  Lambda(lambda x : x[:, :256])(layer1)\n",
        "network1 =  Lambda(lambda x : x[:, 256:])(layer1)\n",
        "\n",
        "\n",
        "network0 = Tea(units=128, name='tea_2_1')(network0)\n",
        "network1 = Tea(units=128, name='tea_2_2')(network1)\n",
        "layer2 = concatenate([network0, network1])\n",
        "\n",
        "# network3 = Lambda(lambda x : x[:, :128])(layer2)\n",
        "# network4 = Lambda(lambda x : x[:, 128:])(layer2)\n",
        "# network3 = Tea(units=128, name='tea_3_1')(network3)\n",
        "# network4 = Tea(units=128, name='tea_3_2')(network4)\n",
        "# layer3 = concatenate([network3, network4])\n",
        "\n",
        "\n",
        "# Gộp đầu ra của 4 core trên lại thành 1 layer khác (tổng sẽ là 256 đầu vào cho tea layer này), tuy nhiên\n",
        "# do chỉ có 10 class (các số từ 0 đến 9, mỗi số 1 class) => layer này chỉ sử dụng 250 neuron (do 256 không\n",
        "# chia hết cho 10), trong đó cứ 25 neuron thì sẽ vote cho 1 class, số lượng neuron vote cho class nào lớn\n",
        "# nhất thì ảnh sẽ thuộc về class đó. Ví dụ trong 250 đầu ra, từ 0 đến 24 có 17 spike bắn ra, đồng thời\n",
        "# không có cụm nào bắn ra được nhiều bằng hoặc hơn 17 => Ảnh thuộc về class số \"0\"\n",
        "layer3 = Tea(units=250, name='tea_3')(layer2)\n",
        "\n",
        "network = AdditivePooling(10)(layer3)\n",
        "\n",
        "predictions = Activation('softmax')(network)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "# Define a learning rate schedule\n",
        "def lr_schedule(epoch):\n",
        "    if epoch <= 5:\n",
        "        return 0.003\n",
        "    if epoch <= 10:\n",
        "        return 0.002\n",
        "    else:\n",
        "        return 0.001\n",
        "# Define the EarlyStopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta =0, patience=30, verbose=1, restore_best_weights=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "X_train = X_train.reshape(-1, 28, 28, 1)\n",
        "X_test = X_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=128, epochs=150, verbose=1, validation_split=0.2, callbacks=[LearningRateScheduler(lr_schedule), early_stopping])\n",
        "\n",
        "# model.fit(X_train, y_train, batch_size=128, epochs=15,verbose=1, validation_split=0.2)\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(\"Test Loss: \", score[0])\n",
        "print(\"Test Accuracy: \", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KR4lv1tqWtrK",
        "outputId": "670b8708-34a8-4ba0-f08c-76320d24d8b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/150\n",
            "47744/48000 [============================>.] - ETA: 0s - loss: 1.5984 - acc: 0.4313"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r48000/48000 [==============================] - 8s 170us/sample - loss: 1.5963 - acc: 0.4322 - val_loss: 1.2794 - val_acc: 0.5346 - lr: 0.0040\n",
            "Epoch 2/150\n",
            "48000/48000 [==============================] - 12s 244us/sample - loss: 1.2583 - acc: 0.5452 - val_loss: 1.2168 - val_acc: 0.5487 - lr: 0.0040\n",
            "Epoch 3/150\n",
            "48000/48000 [==============================] - 5s 113us/sample - loss: 1.1946 - acc: 0.5551 - val_loss: 1.1733 - val_acc: 0.5535 - lr: 0.0040\n",
            "Epoch 4/150\n",
            "48000/48000 [==============================] - 5s 100us/sample - loss: 1.1749 - acc: 0.5595 - val_loss: 1.1613 - val_acc: 0.5567 - lr: 0.0040\n",
            "Epoch 5/150\n",
            "48000/48000 [==============================] - 7s 153us/sample - loss: 1.1600 - acc: 0.5637 - val_loss: 1.1510 - val_acc: 0.5594 - lr: 0.0040\n",
            "Epoch 6/150\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 1.1505 - acc: 0.5654 - val_loss: 1.1507 - val_acc: 0.5623 - lr: 0.0040\n",
            "Epoch 7/150\n",
            "48000/48000 [==============================] - 5s 111us/sample - loss: 1.1319 - acc: 0.5702 - val_loss: 1.1372 - val_acc: 0.5652 - lr: 0.0030\n",
            "Epoch 8/150\n",
            "48000/48000 [==============================] - 7s 143us/sample - loss: 1.1272 - acc: 0.5721 - val_loss: 1.1361 - val_acc: 0.5658 - lr: 0.0030\n",
            "Epoch 9/150\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 1.1234 - acc: 0.5721 - val_loss: 1.1322 - val_acc: 0.5658 - lr: 0.0030\n",
            "Epoch 10/150\n",
            "48000/48000 [==============================] - 6s 135us/sample - loss: 1.1192 - acc: 0.5732 - val_loss: 1.1480 - val_acc: 0.5646 - lr: 0.0030\n",
            "Epoch 11/150\n",
            "48000/48000 [==============================] - 5s 107us/sample - loss: 1.1159 - acc: 0.5738 - val_loss: 1.1458 - val_acc: 0.5646 - lr: 0.0030\n",
            "Epoch 12/150\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 1.1056 - acc: 0.5766 - val_loss: 1.1198 - val_acc: 0.5724 - lr: 0.0020\n",
            "Epoch 13/150\n",
            "48000/48000 [==============================] - 7s 150us/sample - loss: 1.1035 - acc: 0.5764 - val_loss: 1.1267 - val_acc: 0.5673 - lr: 0.0020\n",
            "Epoch 14/150\n",
            "48000/48000 [==============================] - 5s 100us/sample - loss: 1.0987 - acc: 0.5775 - val_loss: 1.1182 - val_acc: 0.5722 - lr: 0.0020\n",
            "Epoch 15/150\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 1.0990 - acc: 0.5775 - val_loss: 1.1175 - val_acc: 0.5698 - lr: 0.0020\n",
            "Epoch 16/150\n",
            "48000/48000 [==============================] - 7s 153us/sample - loss: 1.0958 - acc: 0.5784 - val_loss: 1.1175 - val_acc: 0.5717 - lr: 0.0020\n",
            "Epoch 17/150\n",
            "48000/48000 [==============================] - 6s 118us/sample - loss: 1.0856 - acc: 0.5806 - val_loss: 1.1074 - val_acc: 0.5748 - lr: 0.0010\n",
            "Epoch 18/150\n",
            "48000/48000 [==============================] - 8s 173us/sample - loss: 1.0809 - acc: 0.5817 - val_loss: 1.1082 - val_acc: 0.5742 - lr: 0.0010\n",
            "Epoch 19/150\n",
            "48000/48000 [==============================] - 5s 107us/sample - loss: 1.0803 - acc: 0.5816 - val_loss: 1.1106 - val_acc: 0.5717 - lr: 0.0010\n",
            "Epoch 20/150\n",
            "48000/48000 [==============================] - 5s 106us/sample - loss: 1.0790 - acc: 0.5822 - val_loss: 1.1091 - val_acc: 0.5741 - lr: 0.0010\n",
            "Epoch 21/150\n",
            "48000/48000 [==============================] - 7s 150us/sample - loss: 1.0781 - acc: 0.5824 - val_loss: 1.1150 - val_acc: 0.5745 - lr: 0.0010\n",
            "Epoch 22/150\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 1.0759 - acc: 0.5828 - val_loss: 1.1189 - val_acc: 0.5728 - lr: 0.0010\n",
            "Epoch 23/150\n",
            "48000/48000 [==============================] - 5s 101us/sample - loss: 1.0757 - acc: 0.5828 - val_loss: 1.1105 - val_acc: 0.5729 - lr: 0.0010\n",
            "Epoch 24/150\n",
            "48000/48000 [==============================] - 7s 149us/sample - loss: 1.0766 - acc: 0.5824 - val_loss: 1.1187 - val_acc: 0.5729 - lr: 0.0010\n",
            "Epoch 25/150\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 1.0770 - acc: 0.5825 - val_loss: 1.1132 - val_acc: 0.5739 - lr: 0.0010\n",
            "Epoch 26/150\n",
            "48000/48000 [==============================] - 6s 125us/sample - loss: 1.0735 - acc: 0.5834 - val_loss: 1.1123 - val_acc: 0.5723 - lr: 0.0010\n",
            "Epoch 27/150\n",
            "48000/48000 [==============================] - 6s 124us/sample - loss: 1.0731 - acc: 0.5832 - val_loss: 1.1134 - val_acc: 0.5737 - lr: 0.0010\n",
            "Epoch 28/150\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 1.0730 - acc: 0.5837 - val_loss: 1.1189 - val_acc: 0.5696 - lr: 0.0010\n",
            "Epoch 29/150\n",
            "48000/48000 [==============================] - 7s 156us/sample - loss: 1.0727 - acc: 0.5837 - val_loss: 1.1124 - val_acc: 0.5741 - lr: 0.0010\n",
            "Epoch 30/150\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 1.0735 - acc: 0.5828 - val_loss: 1.1135 - val_acc: 0.5727 - lr: 0.0010\n",
            "Epoch 31/150\n",
            "48000/48000 [==============================] - 5s 102us/sample - loss: 1.0736 - acc: 0.5835 - val_loss: 1.1111 - val_acc: 0.5730 - lr: 0.0010\n",
            "Epoch 32/150\n",
            "41088/48000 [========================>.....] - ETA: 1s - loss: 1.0742 - acc: 0.5825"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-eca889ef370a>\u001b[0m in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLearningRateScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_schedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# model.fit(X_train, y_train, batch_size=128, epochs=15,verbose=1, validation_split=0.2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         return func.fit(\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_sample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         return fit_loop(\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                 \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4605\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4607\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4608\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4609\u001b[0m         output_structure = tf.nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[1;32m   1506\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m                                                run_metadata_ptr)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wz_10mPNYNbh"
      },
      "outputs": [],
      "source": [
        "# Optionally, then save the generated network out for use in the simulator and/or hardware\n",
        "from rancutils.teaconversion import create_cores, create_packets, Packet\n",
        "from rancutils.output_bus import OutputBus\n",
        "from rancutils.serialization import save as sim_save\n",
        "\n",
        "x_test_flat = X_test.reshape((10000, 784))\n",
        "partitioned_packets = []\n",
        "\n",
        "# Use absolute/hard reset by specifying neuron_reset_type=0\n",
        "cores_sim = create_cores(model, 3, neuron_reset_type=0)\n",
        "# Partition the packets into groups as they will be fed into each of the input cores\n",
        "partitioned_packets.append(x_test_flat[:num_test_samples, :256])\n",
        "partitioned_packets.append(x_test_flat[:num_test_samples, 176:432])\n",
        "partitioned_packets.append(x_test_flat[:num_test_samples, 352:608])\n",
        "partitioned_packets.append(x_test_flat[:num_test_samples, 528:])\n",
        "packets_sim = create_packets(partitioned_packets)\n",
        "output_bus_sim = OutputBus((0, 3), num_outputs=250)\n",
        "\n",
        "# This file can then be used as an input json to the RANC Simulator through the \"input file\" argument.\n",
        "sim_save(\"mnist_config.json\", cores_sim, packets_sim, output_bus_sim, indent=2)\n",
        "# Additionally, output the tensorflow predictions and correct labels for later cross validation\n",
        "predict = model.predict(X_test[:num_test_samples,:])\n",
        "idx=[]\n",
        "for i in predict:\n",
        "  idx.append(np.argmax(i))\n",
        "test_predictions = to_categorical(idx)\n",
        "np.save(\"mnist_tf_preds.txt\", test_predictions)\n",
        "np.save(\"mnist_correct_preds.txt\", y_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCtg7WzajcEw"
      },
      "source": [
        "Sửa file neuron trong rancutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP433Amj-kMt"
      },
      "outputs": [],
      "source": [
        "from rancutils.serialization import load\n",
        "from rancutils.emulation import output_for_streaming,output_for_testbench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH8cTHzPgpUv"
      },
      "outputs": [],
      "source": [
        "packets,cores=load(\"mnist_config.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dhjti3Begwzu"
      },
      "outputs": [],
      "source": [
        "output_for_streaming(cores,packets,max_xy=(4,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap33jAPBgulE"
      },
      "outputs": [],
      "source": [
        "correct_outputs=np.load(\"mnist_correct_preds.txt.npy\")\n",
        "output_for_testbench(packets,correct_outputs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}